#!/usr/bin/env python
import os
import re
import math
import htcondor
import argparse
from datetime import date, datetime
from pprint import pprint
from collections import defaultdict
from multiprocessing.pool import Pool

from rebsmearv2.helpers.condor import condor_submit
from rebsmearv2.helpers.git import git_rev_parse, git_diff
from rebsmearv2.helpers.dataset import files_from_eos
from rebsmearv2.helpers.deployment import pack_repo
from rebsmearv2.helpers.paths import xrootd_format, rebsmear_path

from rebsmearv2.rebalance.rebalancer import RebalanceExecutor
from rebsmearv2.smear.smearer import SmearExecutor
from rebsmearv2.postprocess.postProcessor import PostProcExecutor

pjoin = os.path.join

def parse_cli():
    parser = argparse.ArgumentParser()
    parser.add_argument('--outpath', default=f'./output/{date.today().strftime("%Y-%m-%d")}_rebsmear_run', help='Path to save the output under.')
    parser.add_argument('--steps', nargs='*', help='The steps to process. Default is both: rebalance and smear.', default=['rebalance', 'smear'])
    parser.add_argument('--tree',type=str, default='Events', help='Name of the TTree in the input files.')
    parser.add_argument('--jobs','-j', type=int, default=1, help='Number of cores to use / request.')
    parser.add_argument('--dummyjer', help='Placeholder Gaussian width for JER (for testing).', type=float, default=None)
    parser.add_argument('--rebpath', help='The path to the directory including rebalanced trees, if they are already present.', default=None)

    subparsers = parser.add_subparsers(help='sub-command help')
    # Arguments passed to the "run" operation
    parser_run = subparsers.add_parser('run', help='Running help')
    parser_run.add_argument('--dataset', type=str, help='Dataset name to run over.')
    parser_run.add_argument('--testrun', action='store_true', help='Test run: Run over 5 files per dataset and 10 events for each.')
    parser_run.set_defaults(func=do_run)

    # Arguments passed to the "worker" operation
    parser_worker = subparsers.add_parser('worker', help='Running help')
    parser_worker.add_argument('--dataset', type=str, help='Dataset name to run over.')
    parser_worker.add_argument('--filelist', type=str, help='Text file with file names to run over.')
    parser_worker.add_argument('--chunk', type=str, help='Number of this chunk for book keeping.')
    parser_worker.set_defaults(func=do_worker)

    # Arguments passed to the "submit" operation
    parser_submit = subparsers.add_parser('submit', help='Submission help')
    parser_submit.add_argument('--dataset', type=str, help='Dataset regex to use.')
    parser_submit.add_argument('--filesperjob', type=int, default=2, help='Number of files to process per job')
    parser_submit.add_argument('--eventsperjob', type=int, default=5e6, help='Number of events to process per job')
    parser_submit.add_argument('--name', type=str, default=f'{date.today().strftime("%Y-%m-%d")}_rebsmear_run', help='Name to identify this submission')
    parser_submit.add_argument('--dry', action="store_true", default=False, help='Do not trigger submission, just dry run.')
    parser_submit.add_argument('--test', action="store_true", default=False, help='Only run over one file per dataset for testing.')
    parser_submit.add_argument('--asynchronous', action="store_true", default=False, help='Submit asynchronously.')
    parser_submit.add_argument('--memory',type=int, default=None, help='Memory to request (in MB). Default is 2100 * number of cores.')
    parser_submit.set_defaults(func=do_submit)

    args = parser.parse_args()
    return args

def get_jer_source(dataset=''):
    if re.match('QCD.*HT.*', dataset):
        return 'jer_mc'
    return 'jer_data'

def chunk_by_files(items, nchunk):
    '''Split list of items into nchunk ~equal sized chunks'''
    chunks = [[] for _ in range(nchunk)]
    for i in range(len(items)):
        chunks[i % nchunk].append(items[i])
    return chunks

def run_rebsmear_job(files, ichunk, dataset, treename='Events', test=False, jersource=None, outpath=None, rebalanced_tree_dir=None):
    '''
    Wrapper function to run a complete rebalance and smear job.
    Goes through the following steps:
    1. Rebalancing
    2. Apply smearing + accumulate to histograms
    3. Make plots (Yet to come!)

    The steps to run have to be specified in the "steps" argument. By default,
    this function will run both steps.
    '''
    # If this function is provided an input directory with trees containing already rebalanced events,
    # it will skip the rebalancing and directly run the Smearer on these files.
    # Otherwise, execute both steps.
    do_rebalance = True
    if rebalanced_tree_dir:
        do_rebalance = False

    if do_rebalance:
        jersource = get_jer_source(dataset)
        # Step 1: Rebalancing
        e = RebalanceExecutor(files=files,
                ichunk=ichunk, 
                dataset=dataset, 
                treename='Events', 
                test=test,
                jersource=jersource,
            )
            
        # Set output path to save files
        e.set_output_dir(outpath)
        outfiles = e.process()

    # If we don't do rebalancing and run smearing on ready trees,
    # one MUST specify the set of files.
    else:
        outfiles = [pjoin(rebalanced_tree_dir, p) for p in os.listdir(rebalanced_tree_dir) if re.match('.*root', p)]

    print('Rebalancing done')
    print(f'Starting smearing step: {datetime.now().strftime("%H:%M:%S")}')

    # Step 2: Smearing + accumulating
    se = SmearExecutor(files=outfiles, ichunk=ichunk)

    # Set output path to save files
    # Override the the default if we're using existing trees
    # In this case, save the coffea file in the same directory with the ROOT files.
    # if not do_rebalance:
        # outpath = os.path.dirname(outfiles[0])

    se.set_output_dir(outpath)
    se.analyze_files()

    # Step 3: Plotting
    # To come?

def do_run(args):
    """Run the R&S locally."""
    # Run on readily rebalanced trees
    if args.rebpath:
        files = [pjoin(args.rebpath, p) for p in os.listdir(args.rebpath) if re.match('.*root', p)]
        dataset = re.findall('JetHT_ver\d_2017[A-F]', files[0])[0]

        run_rebsmear_job(
            files=files,
            ichunk=0,
            dataset=dataset,
            treename='Events',
            test=args.testrun,
            outpath=args.outpath,
            rebalanced_tree_dir=args.rebpath
        )

    else:
        # Run over all files associated to dataset
        fileset = files_from_eos(regex=args.dataset)
    
        ndatasets = len(fileset)
        nfiles = sum([len(x) for x in fileset.values()])
        print(f"Running over {ndatasets} datasets with a total of {nfiles} files.")
    
        for idx, (dataset, files) in enumerate(fileset.items()):
            run_rebsmear_job(
                files=files,
                ichunk=idx,
                dataset=dataset,
                treename='Events',
                test=args.testrun,
                jersource=get_jer_source(args.dataset),
                outpath=args.outpath,
                rebalanced_tree_dir=args.rebpath
            )

def do_worker(args):
    '''Run the R&S on a worker node.'''
    # Run over all files associated to dataset
    with open(args.filelist, "r") as f:
        files = [xrootd_format(x.strip()) for x in f.readlines()]
    fileset = {args.dataset : files}

    ndatasets = len(fileset)
    nfiles = sum([len(x) for x in fileset.values()])
    print(f"Running over {ndatasets} datasets with a total of {nfiles} files.")

    run_rebsmear_job(
        files=files,
        ichunk=args.chunk,
        dataset=args.dataset,
        treename='Events',
        jersource=get_jer_source(args.dataset),
        outpath=args.outpath,
        rebalanced_tree_dir=args.rebpath
    )

def do_submit(args):
    '''Handle HTcondor submission.'''
    # Extract files per dataset
    dataset_files = files_from_eos(regex=args.dataset)
    # Test mode: 1 file per dataset
    if args.test:
        tmp = {}
        for k, v in dataset_files.items():
            tmp[k] = v[:1]
        dataset_files = tmp
    
    # Output directory for this job
    outdir = f'./submission/{args.name}'
    if not os.path.exists(outdir):
        os.makedirs(outdir)
    
    # Repo version information
    with open(pjoin(outdir, 'version.txt'),'w') as f:
        f.write(git_rev_parse()+'\n')
        f.write(git_diff()+'\n')

    # Sub-directory to store submission files
    filedir = 'files'
    if not os.path.exists(pjoin(outdir, filedir)):
        os.makedirs(pjoin(outdir, filedir))

    input_files = []
    # Pack the repository and ship it with the job
    gridpack_path = pjoin(outdir, 'gridpack.tgz')
    gridpack_exists = os.path.exists(gridpack_path)
    if (not gridpack_exists):
        pack_repo(gridpack_path)
    input_files.append(os.path.abspath(gridpack_path))
    
    if args.asynchronous:
        jdl_to_submit = []

    for dataset, files in dataset_files.items():
        print(f"Writing submission files for dataset: {dataset}.")

        nchunk = math.ceil(len(files)/args.filesperjob)
        chunks = chunk_by_files(files, nchunk=int(nchunk))
        for ichunk, chunk in enumerate(chunks):
            # Save input files to a txt file and send to job
            tmpfile = pjoin(outdir, filedir, f"input_{dataset}_{ichunk:03d}of{len(chunks):03d}.txt")
            with open(tmpfile, "w") as f:
                for file in chunk:
                    f.write(f"{file}\n")

            # Job file creation
            arguments = [
                f'--outpath .',
                f'--jobs {args.jobs}',
                f'--tree {args.tree}',
                'worker',
                f'--dataset {dataset}',
                f'--filelist {os.path.basename(tmpfile)}',
                f'--chunk {ichunk}'
            ]

            job_input_files = input_files + [
                os.path.abspath(tmpfile)
            ]

            if args.rebpath:
                arguments.insert(0, '--rebpath .')
                job_input_files.extend(
                    [pjoin(args.rebpath, p) for p in os.listdir(args.rebpath) if p.endswith('.root')]
                )

            chunkname = f'{dataset}_{ichunk:03d}of{len(chunks):03d}'
            submission_settings = {
                "Initialdir" : outdir,
                "executable": rebsmear_path("execute/htcondor_wrap.sh"),
                "should_transfer_files" : "YES",
                "when_to_transfer_output" : "ON_EXIT",
                "transfer_input_files" : ", ".join(job_input_files),
                # "environment" : '"' + ' '.join([f"{k}={v}" for k, v in environment.items()]) + '"',
                "arguments": " ".join(arguments),
                "Output" : f"{filedir}/out_{chunkname}.txt",
                "Error" : f"{filedir}/err_{chunkname}.txt",
                "log" : f"{filedir}/log_{chunkname}.txt",
                "request_cpus" : str(args.jobs),
                "request_memory" : str(args.memory if args.memory else args.jobs*2100),
                "+MaxRuntime" : f"{60*60*12}",
                # "on_exit_remove" : "((ExitBySignal == False) && (ExitCode == 0)) || (NumJobStarts >= 2)",
            }

            sub = htcondor.Submit(submission_settings)
            jdl = pjoin(outdir,filedir,f'job_{chunkname}.jdl')
            
            with open(jdl,"w") as f:
                f.write(str(sub))
                f.write("\nqueue 1\n")

            # Submission
            if args.dry:
                jobid = -1
                print(f"Submitted job {jobid}")
            else:
                if args.asynchronous:
                    jdl_to_submit.append(jdl)
                else:
                    jobid = condor_submit(jdl)
                    print(f"Submitted job {jobid}")
    
    if args.asynchronous:
        print('Starting asynchronous submission.')
        p = Pool(processes=8)
        res = p.map_async(condor_submit, jdl_to_submit)
        res.wait()
        if res.successful():
            print(f"Asynchronous submission successful for {len(jdl_to_submit)} jobs.")
        else:
            print("Asynchronous submission failed.")

def main():
    args = parse_cli()
    args.func(args)

if __name__ == '__main__':
    main()